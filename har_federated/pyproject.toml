# =====================================================================
# For a full TOML configuration guide, check the Flower docs:
# https://flower.ai/docs/framework/how-to-configure-pyproject-toml.html
# =====================================================================

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[project]
name = "har_distilled"
version = "1.0.0"
description = "UCI HAR Federated Learning with Federated Distillation (Knowledge Distillation)"
license = "Apache-2.0"
# Dependencies for your Flower App
dependencies = [
    "flwr[simulation]>=1.22.0",
    "tensorflow==2.15.0",
    "scikit-learn==1.6.1",
    "pandas>=1.3.0",
    "matplotlib>=3.4.0",
    "seaborn>=0.11.0",
    "numpy>=1.21.0",
    "psutil>=1.0.0"
]

[tool.hatch.build.targets.wheel]
packages = ["."]

[tool.flwr.app]
publisher = "alde"

# Point to your ServerApp and ClientApp objects
# Format: "<module>:<object>"
[tool.flwr.app.components]
serverapp = "har.server_app:app"
clientapp = "har.client_app:app"

[tool.flwr.app.config]
num-server-rounds = 15
local-epochs = 2
batch-size = 32
verbose = false
fraction-fit = 1.0
fraction-evaluate = 0.0  # ‚Üê DISABILITATO: i client non hanno test set locale
compute-aware = false  # üÜï Set to true per compute-aware partitioning
# data-path = "./partitioned_data"  # ‚Üê Opzionale: usa partizioni pre-generate
# experiment-name = "har_baseline"  # ‚Üê Opzionale: nome personalizzato per l'esperimento
num-classes = 6  # UCI HAR ha 6 classi

# üÜï Gradient Compression (Fase 2) - OPTIMIZATION_ROADMAP
compression-type = "none"          # Options: "none", "quantization", "topk"
compression-num-bits = 8           # For quantization: 8 or 16 bits
compression-k-percent = 0.1        # For topk: 0.01-1.0 (10% = 0.1)

# üÜï Early Stopping (Phase 4) - OPTIMIZATION_ROADMAP
local-val-split = 0.2              # Validation split (0.0-0.5, 0.2=20% for validation, enables early stopping)
early-stopping-enabled = true      # Enable/disable early stopping
early-stopping-patience = 3        # Epochs without improvement before stopping
early-stopping-min-delta = 0.001   # Minimum improvement threshold
early-stopping-adaptive = false    # Use adaptive patience (decreases over rounds)

# üéì Federated Distillation (Phase 5) - KNOWLEDGE DISTILLATION
distillation-enabled = false       # Enable/disable Federated Distillation (200x communication reduction)
distillation-batch-size = 100      # Number of unlabeled samples for distillation (10KB vs 2MB weights)
distillation-temperature = 3.0     # Temperature for softmax scaling (1.0-5.0, higher = softer)
distillation-epochs = 5            # Epochs for server-side distillation training
distillation-lr = 0.001            # Learning rate for distillation (0.0001-0.01)

# Default federation to use when running the app
[tool.flwr.federations]
default = "local-deployment"

# Local simulation federation with 2 virtual SuperNodes (for testing distillation)
[tool.flwr.federations.local-simulation]
options.num-supernodes = 3

# Remote federation example for use with SuperLink
[tool.flwr.federations.remote-federation]
address = "192.168.1.121:9093"
insecure = true  # Remove this line to enable TLS
# root-certificates = "<PATH/TO/ca.crt>"  # For TLS setup

[tool.flwr.federations.local-deployment]
address = "127.0.0.1:9093"
insecure = true
options.num-supernodes = 2 
